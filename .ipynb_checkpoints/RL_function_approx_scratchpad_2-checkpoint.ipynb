{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad as compute_grad   # The only autograd function you may ever need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "functions = function_approximators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = functions.fourier_approximator\n",
    "h_grad = compute_grad(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "got here\n",
      "1\n",
      "got there\n",
      "[[-0.95892427  0.28366219  0.28366219]\n",
      " [ 0.0044257   3.99996083  3.99996083]]\n"
     ]
    }
   ],
   "source": [
    "# initial weighting scheme\n",
    "W = np.array([[1,2,3],[4,5,6]])\n",
    "W = W.astype('float')\n",
    "\n",
    "s = 1\n",
    "result = test2(W)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named function_approximators",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-614c7d0d0846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_approximators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named function_approximators"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from function_approximators import function_approximators\n",
    "\n",
    "class learner():\n",
    "    def __init__(self,**args):\n",
    "        # get some crucial parameters from the input gridworld\n",
    "        self.grid = args['gridworld']\n",
    "        \n",
    "        # initialize q-learning params\n",
    "        self.gamma = 1\n",
    "        self.max_steps = 5*self.grid.width*self.grid.height\n",
    "        self.exploit_param = 0.5\n",
    "        self.step_size = 10**-2\n",
    "        self.action_method = 'exploit'\n",
    "        self.training_episodes = 500\n",
    "        self.validation_episodes = 50\n",
    "        self.training_start_schedule = []\n",
    "        self.validation_start_schedule = []\n",
    "        \n",
    "        # swap out for user defined q-learning params if desired\n",
    "        if \"gamma\" in args:\n",
    "            self.gamma = args['gamma']\n",
    "        if 'max_steps' in args:\n",
    "            self.max_steps = args['max_steps']\n",
    "        if 'action_method' in args:\n",
    "            self.action_method = args['action_method']\n",
    "        if 'step_size' in args:\n",
    "            self.step_size = args['step_size']\n",
    "        if 'exploit_param' in args:\n",
    "            self.exploit = args['exploit_param']\n",
    "            self.action_method = 'exploit'\n",
    "        if 'training_episodes' in args:\n",
    "            self.training_episodes = args['training_episodes']\n",
    "            # return error if number of training episodes is too big\n",
    "        if self.training_episodes > self.grid.training_episodes:\n",
    "            print 'requesting too many training episodes, the maximum num = ' + str(self.grid.training_episodes)\n",
    "            return \n",
    "        self.training_start_schedule = self.grid.training_start_schedule[:self.training_episodes]       \n",
    "        if 'validation_episodes' in args:\n",
    "            self.validation_episodes = args['validation_episodes']\n",
    "            # return error if number of training episodes is too big\n",
    "        if self.validation_episodes > self.grid.validation_episodes:\n",
    "            print 'requesting too many validation episodes, the maximum num = ' + str(self.grid.validation_episodes)\n",
    "            return \n",
    "        self.validation_start_schedule = self.grid.validation_start_schedule[:self.validation_episodes]\n",
    "            \n",
    "        ##### import function approximators class #####\n",
    "        functions = function_approximators()  # get instance of function approximator class\n",
    "        \n",
    "        # set default function approximator = linear\n",
    "        self.h = functions.linear_approximator\n",
    "             \n",
    "        # initialize function approximation params and weights\n",
    "        self.deg = 1\n",
    "        if 'degree' in args:\n",
    "            self.deg = args['degree']\n",
    "        \n",
    "        # initialize weight matrix for function approximator\n",
    "        self.num_actions = 4\n",
    "        self.W = np.random.randn(self.deg,1 + 3,self.num_actions)     # the number of weights per function --> 1 outer bias, 1 inner bias, 2 (one per state dim)\n",
    "            \n",
    "        # switch for choosing various nonlinear approximators\n",
    "        approximator = args['approximator']\n",
    "        if approximator == 'fourier':\n",
    "            self.h = functions.fourier_approximator\n",
    "        \n",
    "        # compute gradient of approximator for later use\n",
    "        self.h_grad = compute_grad(self.h)\n",
    "    \n",
    "    \n",
    "    # evaluate a state through our action-based function approximators\n",
    "    def evaluate_h(self,s):\n",
    "        # loop over function approximators and evaluate each one-at-a-time\n",
    "        h_eval = []\n",
    "        for a in range(self.num_actions):\n",
    "            temp = self.h(self.W[:,:,a])\n",
    "            h_eval.append(temp)\n",
    "        return h_eval\n",
    "    \n",
    "    ### Q-learning function - version 1 - take random actions ###\n",
    "    def train(self,**args):\n",
    "        # make local (non-deep) copies of globals for easier reading\n",
    "        grid = self.grid\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        # containers for storing various output\n",
    "        self.training_episodes_history = {}\n",
    "        self.training_reward = []\n",
    "        self.validation_reward = []\n",
    "\n",
    "        ### start main Q-learning loop ###\n",
    "        for n in range(self.training_episodes): \n",
    "            # pick this episode's starting position\n",
    "            grid.agent = self.training_start_schedule[n]\n",
    "            \n",
    "            ### get model functions evaluated at agent current state ###\n",
    "            # get all function approximator evaluations of current state\n",
    "            h_eval = self.evaluate_h(np.copy(grid.agent))\n",
    "                \n",
    "            # update Q matrix while loc != goal\n",
    "            episode_history = []      # container for storing this episode's journey\n",
    "            total_episode_reward = 0\n",
    "            for step in range(self.max_steps):   \n",
    "                # update episode history container\n",
    "                episode_history.append(grid.agent)\n",
    "                \n",
    "                ### if you reach the goal end current episode immediately\n",
    "                if grid.agent == grid.goal:\n",
    "                    break\n",
    "                                   \n",
    "                ### translate current agent location tuple into index\n",
    "                s_k_1 = grid.state_tuple_to_index(grid.agent)\n",
    "                    \n",
    "                ### get action\n",
    "                a_k = grid.get_action(method = self.action_method,h = h_eval,exploit_param = self.exploit_param)\n",
    "                \n",
    "                ### move based on this action\n",
    "                print (a_k)\n",
    "                s_k = grid.get_movin(action = a_k)\n",
    "                \n",
    "                ### update current location of agent \n",
    "                grid.agent = grid.state_index_to_tuple(state_index = s_k)\n",
    "                \n",
    "                ### get reward     \n",
    "                r_k = grid.get_reward(state_index = s_k) \n",
    "                \n",
    "                ### update model params ###\n",
    "                # transform current state using chosen function approximator\n",
    "                h_eval = self.evaluate_h(np.copy(grid.agent))\n",
    "                \n",
    "                # update Q function data\n",
    "                q_k = r_k + gamma*max(h_eval)\n",
    "                \n",
    "                # update model given new datapoint\n",
    "                j = np.argmax(h_eval)\n",
    "                h_j = h_eval[j]\n",
    "                grad = h_grad(W[:,:,j])   \n",
    "                self.W[:,:,j] = self.W[:,:,j] - self.step_size*(h_j - q_k)*grad.flatten()    \n",
    "                \n",
    "                # update training reward\n",
    "                total_episode_reward+=r_k\n",
    "            # print out update if verbose set to True\n",
    "            if 'verbose' in args:\n",
    "                if args['verbose'] == True:\n",
    "                    if np.mod(n+1,50) == 0:\n",
    "                        print 'training episode ' + str(n+1) +  ' of ' + str(self.training_episodes) + ' complete'\n",
    "            \n",
    "            ### store this episode's training reward history\n",
    "            self.training_episodes_history[str(n)] = episode_history\n",
    "            self.training_reward.append(total_episode_reward)\n",
    "            \n",
    "            ### store this episode's validation reward history\n",
    "            if 'validate' in args:\n",
    "                if args['validate'] == True:\n",
    "                    reward = self.validate(Q)\n",
    "                    self.validation_reward.append(reward)\n",
    "            \n",
    "        print 'q-learning algorithm complete'\n",
    "   \n",
    "    ### run validation episodes ###\n",
    "    def validate(self):\n",
    "        # make local (non-deep) copies of globals for easier reading\n",
    "        grid = self.grid\n",
    "        \n",
    "        # run validation episodes\n",
    "        total_reward = []\n",
    "\n",
    "        # run over validation episodes\n",
    "        for i in range(self.validation_episodes):  \n",
    "\n",
    "            # get this episode's starting position\n",
    "            grid.agent = self.validation_start_schedule[i]\n",
    "\n",
    "            # reward container for this episode\n",
    "            episode_reward = 0\n",
    "\n",
    "            # run over steps in single episode\n",
    "            for j in range(grid.max_steps):\n",
    "                ### if you reach the goal end current episode immediately\n",
    "                if grid.agent == grid.goal:\n",
    "                    break\n",
    "                \n",
    "                # evaluate functions at current location\n",
    "                h_eval = self.evaluate_h(np.copy(grid.agent))\n",
    "\n",
    "                # translate current agent location tuple into index\n",
    "                s_k_1 = grid.state_tuple_to_index(grid.agent)\n",
    "                    \n",
    "                # get action\n",
    "                a_k = grid.get_action(method = 'optimal',h = h_eval)\n",
    "                \n",
    "                # move based on this action - if move takes you out of gridworld don't move and instead move randomly \n",
    "                s_k = grid.get_movin(action = a_k, illegal_move_response = 'random')\n",
    "  \n",
    "                # compute reward and save\n",
    "                r_k = grid.get_reward(state_index = s_k)          \n",
    "                episode_reward += r_k\n",
    "    \n",
    "                # update agent location\n",
    "                grid.agent = grid.state_index_to_tuple(state_index = s_k)\n",
    "                \n",
    "            # after each episode append to total reward\n",
    "            total_reward.append(episode_reward)\n",
    "\n",
    "        # return total reward\n",
    "        return np.median(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import gridworld library - make sure this is executed prior to running any gridworld cell\n",
    "import sys\n",
    "sys.path.append('demo_python_backend_files')\n",
    "%matplotlib inline\n",
    "\n",
    "# import custom gridworld simulator \n",
    "from gridworld_lib import gridworld_enviro_func_approx\n",
    "\n",
    "# generate instance of gridworld\n",
    "small_maze = gridworld_enviro_func_approx.environment(world_size = 'small', world_type = 'maze',hazard_reward = -50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-35ad2782e663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# run q-learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mqlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'exploit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-4b3f7a348470>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, **args)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m### move based on this action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0ms_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_movin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m### update current location of agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Nurgetson/Dropbox/github_repos/mlrefined/demo_python_backend_files/gridworld_lib/gridworld_enviro_func_approx.pyc\u001b[0m in \u001b[0;36mget_movin\u001b[0;34m(self, **args)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# update old location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mloc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_choices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# switch for how to deal with the possibility of new state being outside of gridworld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# create an instance of the q-learner\n",
    "qlearner = learner(gridworld = small_maze,approximator = 'linear',degree = 1)\n",
    "\n",
    "# run q-learning\n",
    "qlearner.train(verbose = True, action_method = 'exploit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlearner.num_actions"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
